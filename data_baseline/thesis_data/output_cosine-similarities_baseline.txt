Relevancescore: 0.675801157951355
Document: ('https://simple.wikipedia.org/wiki/Data%20science', '447707', 'Data science is the study of the extraction of knowledge from data. It uses various techniques from many fields, including signal processing, mathematics, probability, machine learning, computer programming, statistics, data engineering, pattern matching, and data visualization, with the goal of extracting useful knowledge from the data. With computer systems able to handle more data, big data is an important aspect of data science.\n\nA person that does data science is called a data scientist. Data scientists solve complicated data problems using mathematics, statistics and computer science, although very good skill in these subjects are not required. However, a data scientist is most likely to be an expert in only one or two of these disciplines, meaning that cross disciplinary teams can be a key component of data science.\n\nGood data scientists are able to apply their skills to achieve many kinds of purposes. Their skills and competencies vary widely.\n\nReferences \n\nComputer science\nStatistics')

Relevancescore: 0.4728858470916748
Document: ('https://simple.wikipedia.org/wiki/Data%20mining', '49697', 'Data mining is a term from computer science. Sometimes it is also called knowledge discovery in databases (KDD). Data mining is about finding new information in a lot of data. The information obtained from data mining is hopefully both new and useful. \n\nIn many cases, data is stored so it can be used later. The data is saved with a goal. For example, a store wants to save what has been bought. They want to do this to know how much they should buy themselves, to have enough to sell later. Saving this information, makes a lot of data. The data is usually saved in a database. The reason why data is saved is called the first use. \n\nLater, the same data can also be used to get other information that was not needed for the first use. The store might want to know now what kind of things people buy together when they buy at the store. (Many people who buy pasta also buy mushrooms for example.) That kind of information is in the data, and is useful, but was not the reason why the data was saved. This information is new and can be useful. It is a second use for the same data. \n\nFinding new information that can also be useful from data, is called data mining.\n\nDifferent kinds of data mining \nFor data, there a lot of different kinds of data mining for getting new information. Usually, prediction is involved. There is uncertainty in the predicted results. The following is based on the observation that there is a small green apple in which we can adjust our data in structural manner. Some of the kinds of data mining are: \n Pattern recognition (Trying to find similarities in the rows in the database, in the form of rules. Small -> green. (Small apples are often green))\n Using a Bayesian network (Trying to make something that can say how the different data attributes are connected/influence each other. The size and the colour are related. So if you know something about the size, you can guess the colour.)\n Using a Neural network (Trying to make a model like a brain, which is hard to understand, but a computer can tell that if the apple is green it has a higher chance to be sour, if we tell the computer the apple is green. So this is like a black box model, we do not know how it works, but it works.) \n Using Classification tree (With all other knowledge trying to say what one other thing about the thing we are looking at will be. Here is an apple with a size, a colour and shininess, what will it taste like?)\n\nComputer science')

Relevancescore: 0.4104044437408447
Document: ('https://simple.wikipedia.org/wiki/Statistics', '789', 'Statistics is a branch of applied mathematics that deals with collecting, organising, analysing, reading and presenting data. Descriptive statistics make summaries of data. Inferential statistics makes predictions. Statistics helps in the study of many other fields, such as science, medicine, economics, psychology, politics and marketing. Someone who works in statistics is called a statistician. In addition to being the name of a field of study, the word "statistics"  can also mean numbers that are used to describe data or relationships.\n\nHistory \nThe first known statistics are census data. The Babylonians did a census around 3500 BC, the Egyptians around 2500 BC, and the Ancient Chinese around 1000 BC.\n\nStarting in the 16th century mathematicians such as Gerolamo Cardano developed probability theory, which made statistics a science. Since then, people have collected and studied statistics on many things. Trees, starfish, stars, rocks, words, almost anything that can be counted has been a subject of statistics.\n\nCollecting data \nBefore we can describe the world with statistics, we must collect data. The data that we collect in statistics are called measurements. After we collect data, we use one or more numbers to describe each observation or measurement. For example, suppose that we want to find out how popular a certain TV show is. We can pick a group of people (called a sample) out of the total population of viewers. Then we ask each viewer in the sample how often they watch the show. The sample is data that one can see, and the population is data that one cannot see (assuming that not every viewer in the population are asked). For another example, if we want to know whether a certain drug can help lower blood pressure, we could give the drug to people for some time and measure their blood pressure before and after.\n\nDescriptive and inferential statistics \nNumbers that describe the data one can see are called descriptive statistics. Numbers that make predictions about the data one cannot see are called inferential statistics.\n\nDescriptive statistics involves using numbers to describe features of data. For example, the average height of women in the United States is a descriptive statistic: it describes a feature (average height) of a population (women in the United States).\n\nOnce the results have been summarized and described, they can be used for prediction. This is called inferential statistics. As an example, the size of an animal is dependent on many factors. Some of these factors are controlled by the environment, but others are by inheritance. A biologist might therefore make a model that says that there is a high probability that the offspring will be small in size—if the parents were small in size. This model probably allows to predict the size in better ways than by just guessing at random. Testing whether a certain drug can be used to cure a certain condition or disease is usually done by comparing the results of people who are given the drug against those who are given a placebo.\n\nMethods \nMost often, we collect statistical data by doing surveys or experiments. For example, an opinion poll is one kind of survey. We pick a small number of people and ask them questions. Then, we use their answers as the data.\n\nThe choice of which individuals to take for a survey or data collection is important, as it directly influences the statistics. When the statistics are done, it can no longer be determined which individuals are taken. Suppose we want to measure the water quality of a big lake. If we take samples next to the waste drain, we will get different results than if the samples are taken in a far-away and hard-to-reach spot of the lake.\n\nThere are two kinds of problems which are commonly found when taking samples:\n\n If there are many samples, the samples will likely be very close to what they are in the real population. If there are very few samples, however, they might be very different from what they are in the real population.  This error is called a chance error (see also Errors and residuals in statistics).\n The individuals for the samples need to be chosen carefully. Usually, they will be chosen randomly. If this is not the case, the samples might be very different from what they really are in the total population. This is true even if a great number of samples is taken. This kind of error is called bias.\n\nErrors \nWe can reduce chance errors by taking a larger sample, and we can avoid some bias by choosing randomly. However, sometimes large random samples are hard to take. And bias can happen if different people are not asked, or refuse to answer our questions, or if they know they are getting a fake treatment. These problems can be hard to fix. See standard error for more.\n\nDescriptive statistics')

Relevancescore: 0.4048950672149658
Document: ('https://simple.wikipedia.org/wiki/Analytics', '747593', 'Analytics is the systematic procession of data or statistics. The process of discovering, interpreting, and communicating significant patterns in data to get meaningful information.\n\nIn simple language, analytics is turning raw data into useful one for making better decisions. Analytics uses the application of statistics, computer programming, and operations research to gain information from meanings of data.\n\nReferences \n\nStatistics')

Relevancescore: 0.3921099901199341
Document: ('https://simple.wikipedia.org/wiki/Cluster%20analysis', '593732', 'Clustering or cluster analysis is a type of data analysis. The analyst groups objects so that objects in the same group (called a cluster) are more similar to each other than to objects in other groups (clusters) in some way. This is a common task in data mining.\n\nStatistics')

Relevancescore: 0.3907177448272705
Document: ('https://simple.wikipedia.org/wiki/Statistics', '789', 'Descriptive statistics\n\nFinding the middle of the data \nThe middle of the data is called an average. The average tells us about a typical individual in the population. There are three kinds of average that are often used: the mean, the median and the mode.\n\nThe examples below use this sample data:\n\nMean \nThe formula for the mean is\n\nWhere  are the data and  is the population size (see also Sigma Notation).\n\nThis means that one calculates the mean by adding up all the values, and then divide by the number of values. For the example above, the mean is:\n\nThe problem with the mean is that it does not tell anything about how the values are distributed. Values that are very large or very small change the mean a lot. In statistics, these extreme values might be errors of measurement, but sometimes the population really does contain these values. For example, if there are 10 people in a room who make $10 per day and 1 who makes $1,000,000 per day. The mean of the data is $90,918 per day. Even though it is the average amount, the mean in this case is not the amount any single person makes, and thus is not very useful for some purposes. \n\nThe mean described above is the "arithmetic mean". Other kinds are useful for some purposes.\n\nMedian \nThe median is the middle item of the data. For a given data , this is sometimes written as . To find the median, we sort the data from the smallest number to the largest number, and then choose the number in the middle. If there is an even number of data, there will not be a number right in the middle, so we choose the two middle ones and calculate their mean. In our example above, there are 10 items of data, the two middle ones are "57" and "64", so the median is (57+64)/2 = 60.5.  \n\nAs another example, like the income example presented for the mean, consider a room with 10 people who have incomes of $10, $20, $20, $40, $50, $60, $90, $90, $100, and $1,000,000. Here, the median is $55, because $55 is the average of the two middle numbers, $50 and $60.  If the extreme value of $1,000,000 is ignored, the mean is $53.  In this case, the median is close to the value obtained when the extreme value is thrown out.  The median solves the problem of extreme values as described in the definition of mean above.\n\nMode \nThe mode is the most frequent item of data. For example, the most common letter in English is the letter "e". We would say that "e" is the mode of the distribution of the letters.\n\nAs another example, if there are 10 people in a room with incomes of $10, $20, $20, $40, $50, $60, $90, $90, $90, $100, and $1,000,000, then the mode is $90, because $90 occurs three times and all other values occur fewer than three times.\n\nThere can be more than one mode. For example, if there are 10 people in a room with incomes of $10, $20, $20, $20, $50, $60, $90, $90, $90, $100, and $1,000,000, the modes are $20 and $90. This is bi-modal, or has two modes. Bi-modality is very common, and it often indicates that the data is the combination of two different groups.  For instance, the average height of all adults in the U.S. has a bi-modal distribution.  This is because males and females have separate average heights of 1.763 m (5\xa0ft 9 + 1⁄2 in) for men and 1.622 m (5\xa0ft 4 in) for women.  These peaks are apparent when both groups are combined.\n\nThe mode is the only form of average that can be used for data that can not be put in order.\n\nFinding the spread of the data \nAnother thing we can say about a set of data is how spread out it is. A common way to describe the spread of a set of data is the standard deviation. If the standard deviation of a set of data is small, then most of the data is very close to the average. If the standard deviation is large, though, then a lot of the data is very different from the average.\n\nThe standard deviation of a sample is generally different from the standard deviation of its originating population . Because of that, we write  for population standard deviation, and  for sample standard deviation.\n\nIf the data follows the common pattern called the normal distribution, then it is very useful to know the standard deviation. If the data follows this pattern (we would say the data is normally distributed), about 68 of every 100 pieces of data will be off the average by less than the standard deviation. Not only that, but about 95 of every 100 measurements will be off the average by less than two times the standard deviation, and about 997 in 1000 will be closer to the average by less than three standard deviations.\n\nOther descriptive statistics \nWe also can use statistics to find out that some percent, percentile, number, or fraction of people or things in a group do something or fit in a certain category.\n\nFor example, social scientists used statistics to find out that 49% of people in the world are males.\n\nRelated software\nIn order to support statisticians, many statistical software have been developed:\n\n MATLAB\n R\n SAS Institute\n SPSS (made by IBM)\n\nReferences')

Relevancescore: 0.38608628511428833
Document: ('https://simple.wikipedia.org/wiki/Computer%20science', '110', 'Computer science at work \n Artificial intelligence (making computers learn and talk, similar to people)\n Computer architecture (building a computer)\n Computer graphics (making pictures with computers)\n Computer networks (joining computers to other computers)\n Computer program (how to tell a computer to do something)\n Computer programming (writing, or making, computer programs)\n Computer security (making computers and their data safe)\n Databases (a way to sort and keep data)\n Data structure (how to build or group data)\n Distributed computing (using more than one computer to solve a difficult problem)\n Information retrieval (getting data back from a computer)\n Programming languages (languages that a programmer uses to make computer programs)\n Program specification (what a program is supposed to do)\n Program verification (making sure a computer program does what it should do, see debugging)\n Robots (using computers to control machines)\n Software engineering (how programmers write programs)\n\nWhat computer science does \n Benchmark (testing a computer\'s power or speed)\n Computer vision (how computers can see and understand images)\n Collision detection (how computers help robots move without hitting something)\n Data compression (making data smaller)\n Data structures (how computers group and sort data)\n Data acquisition (putting data into computers)\n Design patterns (answers to common software engineering problems)\n Digital signal processing (cleaning and "looking" at data)\n File formats (how a file is arranged)\n Human-computer interaction (how humans use computers)\n Information security (keeping data safe from other people)\n Internet (a large network that joins almost all computers)\n Web applications (computer programs on the Internet)\n Optimization (making computer programs work better)\n Software metrics (ways to measure computer programs, such as counting lines of code or number of operations)\n VLSI design (the making of a very large and complex computer system)\n\nRelated pages\n Computing\n Formal language\n Turing Award\n Computer jargon\n Encyclopedia of Computer Terms\n\nReferences')

Relevancescore: 0.37238436937332153
Document: ('https://simple.wikipedia.org/wiki/Big%20data', '564597', 'Big data\xa0is a term used for\xa0certain database systems. It is used for a number of\xa0technologies\xa0which help to organize, gather and analyse data. \n\nAt least some of the following characteristics apply:\n There are huge amounts of data\n\n The data changes very often\n\n It is difficult to structure the data so that it can be used easily\n\nUse \nBig data is used to analyse different subjects. Through the analysis, new information can be gained. Bigger amounts of data make it easier to find reliable information. It is used in many different areas, such as government, health care, insurance, media, advertisement and information technology.\n\nOrigin \nThe data is gathered among other things through:\n\n Surveillance\n the use of credit cards\n Electronic Communication (for example E-mail and smartphones)\n Internet and social media (for example from using Google search or when visiting Facebook)\n Internet of things (objects that can connect to the internet, for example Amazon Alexa)\n governments and public authorities (for example tax data, census data)\n\nCritique \nBig data has been criticised for different reasons. One prominent criticism is the increasing surveillance to gather data, which takes place in many new forms. Edward Snowden has revealed how the American National Security Agency (NSA) uses digital technology to spy on people around the world. Another prominent criticisms is data privacy, which is about the risk of sensitive personal data leaking because it is not protected well enough.\n\nA more fundamental critique of big data is just because it is bigger, it is not automatically better. The quality of the data still has to be controlled. It also says that data analysis can only ask "what" is happening, but not "why" it is happening.\n\nDatabases\nStatistics\nPrivacy')

Relevancescore: 0.364459753036499
Document: ('https://simple.wikipedia.org/wiki/Machine%20learning', '564928', 'Machine learning gives computers the ability to learn without being explicitly programmed (Arthur Samuel, 1959). It is a subfield of computer science.\n\nThe idea came from work in artificial intelligence. Machine learning explores the study and construction of algorithms which can learn and make predictions on data. Such algorithms follow programmed instructions, but can also make predictions or decisions based on data. They build a model from sample inputs.\n\nMachine learning is done where designing and programming explicit algorithms cannot be done. Examples include spam filtering, detection of network intruders or malicious insiders working towards a data breach, optical character recognition (OCR), search engines and computer vision.\n\nUsing machine learning has risks. Some algorithms create a final model which is a black box. Models have been criticized for biases in hiring, criminal justice, and recognizing faces.\n\nReferences \n\nArtificial intelligence\nLearning')

Relevancescore: 0.3582957983016968
Document: ('https://simple.wikipedia.org/wiki/Supervised%20learning', '359370', 'In machine learning, supervised learning is the task of inferring a function from labelled training data. The results of the training are known beforehand, the system simply learns how to get to these results correctly. Usually, such systems work with vectors. They get the training data and the result of the training as two vectors and produce a classifier. Usually, the system uses inductive reasoning to generalize the training data.\n\nArtificial intelligence')

