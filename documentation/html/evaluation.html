<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
    <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
        <title>Evaluation &mdash; Flashcard Generator 1.0.0 documentation</title>
    
    <link rel="stylesheet" type="text/css" href="_static/dist/fontawesome.css" />
      <link rel="stylesheet" type="text/css" href="_static/dist/theme.css?v=ea6a6c20" />
            <link rel="index" title="Index" href="genindex.html" />
            <link rel="search" title="Search" href="search.html" />
            <link rel="top" title="Flashcard Generator 1.0.0 documentation" href="#" />
            <link rel="next" title="Utils" href="utils.html" />
            <link rel="prev" title="Generation" href="generation.html" />
    </head>
<body>
    <script type="text/javascript" src="_static/dist/blocking.js"></script>
    <header class="container-fluid bg-primary">
        <a class="btn btn-sm btn-light skip-to-content-link" href="#main">Skip to content</a>
        <div class="container-fluid">
            <div class="navbar navbar-expand-lg navbar-dark font-weight-bold">
                    <a href="index.html"
                        title="FlashcardGenerator"
                        class="logo navbar-brand"
                    >
                        <img src="_static/img/logo_flashcard_generator_small.png" width="45" height="59" alt="FlashcardGenerator"
                            class="logo-img"
                        />
                        FLASHCARD GENERATOR
                    </a>
                
                
                <button class="navbar-toggler btn btn-primary d-lg-none" type="button" data-toggle="collapse" data-target="#collapseSidebar" aria-expanded="false" aria-controls="collapseExample">
                    <span class="navbar-toggler-icon"></span>
                    <span class="sr-only">menu</span>
                </button>
            </div>
        </div>
    </header>
    <div class="container-fluid">
        <div class="row">
            <aside class="col-12 col-lg-3 sidebar-container">
                <div id="collapseSidebar" class="collapse sticky-top d-lg-block pt-5 pr-lg-4">
<div id="searchbox" class="searchbox mb-6 px-1" role="search">
    <form id="search-form" action="search.html" autocomplete="off" method="get" role="search">
        <div class="input-group">
            <div class="input-group-prepend">
                <div class="input-group-text border-right-0 bg-white py-3 pl-3 pr-2"><span class="fas fa-search"></span></div>
            </div>
            <input class="form-control py-3 pr-3 pl-1 h-100 border-left-0" type="search" name="q" placeholder="Search documentation" aria-label="Search documentation" id="searchinput" />
        </div>
        <input type="hidden" name="check_keywords" value="yes" />
        <input type="hidden" name="area" value="default" />
    </form>
</div><div class="site-toc">
    <nav class="toc mt-3" aria-label="Main menu">
        <p class="caption" role="heading"><span class="caption-text">Table of Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="data_preparation.html">Data Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_storage.html">Data Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="retriever.html">Retriever</a></li>
<li class="toctree-l1"><a class="reference internal" href="generation.html">Generation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">Utils</a></li>
</ul>

    </nav>
    <template data-toggle-item-template>
        <button class="btn btn-sm btn-link toctree-expand" type="button">
            <span class="sr-only">Toggle menu contents</span>
        </button>
    </template>
</div>
                    <div class="d-lg-none border-bottom">
                        
                    </div>
                </div>
            </aside>
            <div class="col-12 col-lg-9 pt-5">
                <header class="row align-items-baseline">
                    <div class="col">
                        <nav aria-label="breadcrumb">
    <ol class="breadcrumb m-0 p-0 bg-transparent">
        <li class="breadcrumb-item"><a href="index.html">Docs</a></li>
        <li class="breadcrumb-item active" aria-current="page">Evaluation</li>
    </ol>
</nav>
                    </div>
                    <div class="col-sm-12 col-lg-auto mt-3 mt-lg-3">
                        <noscript>
                            <p>JavaScript is required to toggle light/dark mode..</p>
                        </noscript>
                        <button id="wagtail-theme" class="btn btn-sm btn-light text-decoration-none" type="button">
                            <span class="dark-only"><i class="fas fa-sun"></i> Light mode</span>
                            <span class="light-only"><i class="fas fa-moon"></i> Dark mode</span>
                        </button>
    <a class="btn btn-sm btn-light text-decoration-none" href="https://github.com/wagtail/sphinx_wagtail_theme/blob/main/docs/evaluation.rst" rel="nofollow">
        <span class="btn-icon"><span class="fab fa-github"></span></span>
        <span class="btn-text">Edit on GitHub</span>
    </a>
    <a class="btn btn-sm btn-light text-decoration-none" href="_sources/evaluation.rst.txt" rel="nofollow">
        <span class="btn-icon"><span class="fas fa-code"></span></span>
        <span class="btn-text">View source</span>
    </a>
                        
                    </div>
                </header>
                <div class="row" >
                    <div class="col-12">
                        <hr class="w-100 my-4">
                    </div>
                </div>
                <div class="row">
                    <div class="col-12 col-lg-9 order-last order-lg-first rst-content">
                        <main role="main" id="main">
    <section id="evaluation">
<h1>Evaluation<a class="headerlink" href="#evaluation" title="Link to this heading">¶</a></h1>
<p>This section covers the evaluation methods for the RAG pipeline, utilizing Ragas, DeepEval, and Haystack metrics to assess performance and effectiveness.</p>
<dl class="py function" id="module-tools.pipeline">
<dt class="sig sig-object py" id="tools.pipeline.deepeval_evaluate_data">
<span class="sig-prename descclassname"><span class="pre">tools.pipeline.</span></span><span class="sig-name descname"><span class="pre">deepeval_evaluate_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">deepeval_data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">LLMTestCase</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ground_truth</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'gpt-4'</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="_modules/tools/pipeline.html#deepeval_evaluate_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tools.pipeline.deepeval_evaluate_data" title="Link to this definition">¶</a></dt>
<dd><p>Evaluates a prepared dataset using a suite of DeepEval’s metrics designed to assess the performance of language models. 
It outputs a structured report summarizing the results of each metric evaluation based on whether ground truth is considered.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>deepeval_data</strong> – The dataset containing test cases prepared for evaluation.</p></li>
<li><p><strong>ground_truth</strong> – Specifies whether metrics that require ground truth data should be included.</p></li>
<li><p><strong>model</strong> – The name of the model to be used for evaluation, defaults to “gpt-4”.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Outputs a structured report summarizing the metric evaluations.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tools.pipeline.deepeval_prepare_data">
<span class="sig-prename descclassname"><span class="pre">tools.pipeline.</span></span><span class="sig-name descname"><span class="pre">deepeval_prepare_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">DataFrame</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ground_truth</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">LLMTestCase</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/tools/pipeline.html#deepeval_prepare_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tools.pipeline.deepeval_prepare_data" title="Link to this definition">¶</a></dt>
<dd><p>Prepares a dataset for using DeepEval evaluation metrics by creating test cases for each entry in the dataset. 
Each test case is structured to include a question, the actual and expected answers (if ground truth is provided),
and the retrieval context.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> – The dataset to be prepared, typically containing ‘question’, ‘answer’, and ‘contexts’ columns,
and optionally a ‘ground_truth’ column if ground_truth is True.</p></li>
<li><p><strong>ground_truth</strong> – Specifies whether the ‘ground_truth’ column should be included in the test cases.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A list of test cases, each designed for evaluation purposes.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tools.pipeline.evaluate_retriever">
<span class="sig-prename descclassname"><span class="pre">tools.pipeline.</span></span><span class="sig-name descname"><span class="pre">evaluate_retriever</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">topic</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vectorstore</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Chroma</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">10</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">DataFrame</span></span></span><a class="reference internal" href="_modules/tools/pipeline.html#evaluate_retriever"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tools.pipeline.evaluate_retriever" title="Link to this definition">¶</a></dt>
<dd><p>Evaluates the performance of a vector store’s retrieval capabilities by conducting searches using
both cosine similarity and cosine distance metrics, and then compares the results.</p>
<p>This function retrieves the top ‘k’ documents based on the cosine similarity and cosine distance
metrics for a given topic. It constructs a DataFrame containing the results, including each
document’s ID, URL, context, similarity score, and distance score.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>topic</strong> – The topic string based on which the documents are retrieved.</p></li>
<li><p><strong>vectorstore</strong> – An instance of a vector store configured with embeddings used for retrieving documents.</p></li>
<li><p><strong>k</strong> – The number of top documents to retrieve, defaults to 10.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A pandas DataFrame containing the document details and their respective retrieval scores.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tools.pipeline.haystack_evaluate_data">
<span class="sig-prename descclassname"><span class="pre">tools.pipeline.</span></span><span class="sig-name descname"><span class="pre">haystack_evaluate_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ground_truth</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">DataFrame</span></span></span><a class="reference internal" href="_modules/tools/pipeline.html#haystack_evaluate_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tools.pipeline.haystack_evaluate_data" title="Link to this definition">¶</a></dt>
<dd><p>Evaluates the prepared dataset by running it through a series of Haystack’s evaluation metrics within a
pipeline. Metrics include context relevance, faithfulness of the answers, and optionally
semantic answer similarity if ground_truth is True.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – A list of dictionaries where each dictionary contains ‘questions’, ‘contexts’,
‘predicted_answers’, and optionally ‘ground_truth_answers’ if ground_truth is True.</p></li>
<li><p><strong>ground_truth</strong> – A flag to determine whether semantic similarity evaluation should be
performed using the ground truth answers provided in the data.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A pandas DataFrame containing the evaluation results for each data point, including scores
for context relevance, faithfulness, and optionally semantic similarity along with the overall
scores for these metrics.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tools.pipeline.haystack_prepare_data">
<span class="sig-prename descclassname"><span class="pre">tools.pipeline.</span></span><span class="sig-name descname"><span class="pre">haystack_prepare_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">DataFrame</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ground_truth</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/tools/pipeline.html#haystack_prepare_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tools.pipeline.haystack_prepare_data" title="Link to this definition">¶</a></dt>
<dd><p>Prepares a dataset for using Haystack’s evaluation metrics by formatting the input data into a
specific structure.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> – The dataset to be prepared, typically containing ‘question’, ‘answer’, and ‘contexts’ columns,
and optionally a ‘ground_truth’ column if ground_truth is True.</p></li>
<li><p><strong>ground_truth</strong> – Specifies whether metrics that require ground truth data should be included.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A list of dictionaries, each containing data for a single instance. Each dictionary includes
the question, predicted answers, and context(s). If <cite>ground_truth</cite> is True, it also includes
ground truth answers.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tools.pipeline.ragas_evaluate_data">
<span class="sig-prename descclassname"><span class="pre">tools.pipeline.</span></span><span class="sig-name descname"><span class="pre">ragas_evaluate_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ChatOpenAI_model_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'gpt-4'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ground_truth</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="_modules/tools/pipeline.html#ragas_evaluate_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tools.pipeline.ragas_evaluate_data" title="Link to this definition">¶</a></dt>
<dd><p>Evaluates the prepared dataset using various evaluadtion metrics with a specified ChatGPT model as additional large language model.
It wraps the ChatGPT model in a LangchainLLMWrapper and calculates different metrics based on whether ground truth is considered.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – The prepared dataset to evaluate.</p></li>
<li><p><strong>ChatOpenAI_model_name</strong> – The model name of the ChatGPT to use for evaluation.</p></li>
<li><p><strong>temperature</strong> – The sampling temperature to use in the model inference, defaults to 0.</p></li>
<li><p><strong>ground_truth</strong> – Specifies whether metrics that require ground truth data should be included.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The evaluation results with specified metrics.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="tools.pipeline.ragas_prepare_data">
<span class="sig-prename descclassname"><span class="pre">tools.pipeline.</span></span><span class="sig-name descname"><span class="pre">ragas_prepare_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">DataFrame</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ground_truth</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dataset</span></span></span><a class="reference internal" href="_modules/tools/pipeline.html#ragas_prepare_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tools.pipeline.ragas_prepare_data" title="Link to this definition">¶</a></dt>
<dd><p>Prepares a dataset for use of the Ragas evaluation metrics by transforming
and ensuring proper formatting of its columns. This function resets the dataset index, ensures
‘contexts’ are lists of strings, optionally adds a ‘ground_truth’ field, removes unnecessary
columns, and casts the dataset to specified features.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> – The dataset to prepare, which should contain at least the ‘question’, ‘answer’, ‘contexts’ and ‘source’ field.</p></li>
<li><p><strong>ground_truth</strong> – If True, includes the ‘ground_truth’ field in the final dataset.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The transformed dataset with the specified features format.</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If the data type of the ‘contexts’ field is neither a string nor a list.</p>
</dd>
</dl>
</dd></dl>

</section>

</main>
                        <nav aria-label="Page navigation" class="py-4 my-5 clearfix font-weight-bold border-top">
    <a class="float-left" href="generation.html" title="Previous">
        <span aria-hidden="true">←&nbsp;</span>Generation
    </a>
    <a class="float-right" href="utils.html" title="Next">
        Utils <span aria-hidden="true">&nbsp;→</span>
    </a>
</nav>
                    </div>
                    
                    <nav class="col-12 col-lg-3 pb-4">
                        <div class="sticky-top toc page-toc" aria-labelledby="page-toc-heading">
                            <p class="font-weight-bold" id="page-toc-heading">Page contents</p>
                            <ul>
<li><a class="reference internal" href="#">Evaluation</a><ul>
<li><a class="reference internal" href="#tools.pipeline.deepeval_evaluate_data"><code class="docutils literal notranslate"><span class="pre">deepeval_evaluate_data()</span></code></a></li>
<li><a class="reference internal" href="#tools.pipeline.deepeval_prepare_data"><code class="docutils literal notranslate"><span class="pre">deepeval_prepare_data()</span></code></a></li>
<li><a class="reference internal" href="#tools.pipeline.evaluate_retriever"><code class="docutils literal notranslate"><span class="pre">evaluate_retriever()</span></code></a></li>
<li><a class="reference internal" href="#tools.pipeline.haystack_evaluate_data"><code class="docutils literal notranslate"><span class="pre">haystack_evaluate_data()</span></code></a></li>
<li><a class="reference internal" href="#tools.pipeline.haystack_prepare_data"><code class="docutils literal notranslate"><span class="pre">haystack_prepare_data()</span></code></a></li>
<li><a class="reference internal" href="#tools.pipeline.ragas_evaluate_data"><code class="docutils literal notranslate"><span class="pre">ragas_evaluate_data()</span></code></a></li>
<li><a class="reference internal" href="#tools.pipeline.ragas_prepare_data"><code class="docutils literal notranslate"><span class="pre">ragas_prepare_data()</span></code></a></li>
</ul>
</li>
</ul>

                        </div>
                    </nav>
                    
                </div>
            </div>
        </div>
    </div>
    <footer class="container-fluid bg-primary text-light">
        <div class="container">
            <div class="row">
        <div class="col p-4">
            
                <nav aria-label="Footer">
                    <ul class="nav justify-content-center mb-2">
                        
                            
                            
                            <li class="nav-item"><a class="nav-link text-light"  href="https://wagtail.org/features/">Features</a></li>
                        
                            
                            
                            <li class="nav-item"><a class="nav-link text-light"  href="https://wagtail.org/about-wagtail/"> About Wagtail</a></li>
                        
                            
                            
                            <li class="nav-item"><a class="nav-link text-light"  href="https://wagtail.org/services/"> Services</a></li>
                        
                            
                            
                            <li class="nav-item"><a class="nav-link text-light"  href="https://wagtail.org/blog/"> Blog</a></li>
                        
                            
                            
                            <li class="nav-item"><a class="nav-link text-light"  href="https://wagtail.org/packages/"> Packages</a></li>
                        
                            
                            
                            <li class="nav-item"><a class="nav-link text-light"  href="https://wagtail.org/developers/"> Developers</a></li>
                        
                    </ul>
                </nav>
            
            <div class="text-center">
                <p style="display: none">
                    <a class="text-light" href="https://github.com/wagtail/sphinx_wagtail_theme" rel="nofollow" target="_blank">
                        Wagtail Sphinx Theme 6.3.0
                    </a>
                </p>
            </div>
            <div class="text-center">
                    &copy; Copyright 2024, Christina Schlager
            </div>
        </div>
    </div>
        </div>
    </footer>
        <script src="_static/documentation_options.js?v=8d563738"></script>
        <script src="_static/doctools.js?v=9a2dae69"></script>
        <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
        <script type="text/javascript" src="_static/dist/theme.js"></script>
        <script type="text/javascript" src="_static/dist/vendor.js"></script>
        <script type="text/javascript" src="_static/searchtools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script type="text/javascript">
            document.addEventListener('DOMContentLoaded', function() { Search.loadIndex("searchindex.js"); });
        </script>
        <script type="text/javascript" id="searchindexloader"></script>
    </body>
</html>